{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea1ad02",
   "metadata": {},
   "source": [
    "# Day 1 - Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook explores the **Credit Card Fraud Detection dataset** from Kaggle.  \n",
    "Goal: Understand dataset structure, fraud distribution, and basic feature properties before augmentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f898ee2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Keep plots styled\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv(\"data/creditcard.csv\")  # adjust path if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29e7c7",
   "metadata": {},
   "source": [
    "### 1. Dataset Overview\n",
    "\n",
    "We start by checking the shape, columns, and datatypes of the dataset.  \n",
    "This ensures we understand the raw structure before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634f59e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nDtypes:\\n\", df.dtypes.value_counts())\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d2e53",
   "metadata": {},
   "source": [
    "## 2. Fraud Distribution\n",
    "\n",
    "The target column is **Class**:  \n",
    "- `0` → Not Fraud  \n",
    "- `1` → Fraud  \n",
    "\n",
    "Let’s check how imbalanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876751d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_counts = df['Class'].value_counts()\n",
    "class_percent = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Counts:\\n\", class_counts)\n",
    "print(\"\\nPercentages:\\n\", class_percent)\n",
    "\n",
    "# Bar plot\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"pastel\")\n",
    "plt.xticks([0,1], ['Not Fraud (0)', 'Fraud (1)'])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Fraud Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Pie chart\n",
    "plt.pie(class_counts, labels=['Not Fraud', 'Fraud'], autopct='%1.3f%%', startangle=90, colors=[\"#66b3ff\",\"#ff9999\"])\n",
    "plt.title(\"Fraud vs Not Fraud\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d4f65",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics\n",
    "\n",
    "We’ll generate summary statistics to spot unusual ranges or outliers, especially in `Amount` and the PCA features (`V1`–`V28`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31629c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6aba35",
   "metadata": {},
   "source": [
    "## 4. Correlation Heatmap\n",
    "\n",
    "Finally, let’s visualize correlations between features.  \n",
    "This can hint at which features may carry strong signals for fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbee46a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), cmap=\"coolwarm\", center=0, cbar=True)\n",
    "plt.title(\"Correlation Heatmap\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a3c57",
   "metadata": {},
   "source": [
    "---\n",
    "# Findings (Day 1)\n",
    "\n",
    "- Dataset contains ~285k transactions, with 492 fraud cases (**0.17% fraud**).  \n",
    "- No missing values detected.  \n",
    "- Features are mostly PCA-transformed (`V1–V28`), plus `Time`, `Amount`, and target `Class`.  \n",
    "- Class imbalance is extreme → augmentation will be required before modeling.  \n",
    "- Some features (like `V14`, `V17`) show higher correlation with `Class`.  \n",
    "\n",
    "✅ Next Step (Day 2): Begin **data augmentation** with Faker and resampling strategies to balance fraud vs non-fraud."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
